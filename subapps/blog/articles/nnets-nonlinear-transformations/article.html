<main>
    <header id=content-header>
        <div><strong>Francis Billones, <a href=http://twitter.com/francishubertdb>@francishubertdb</a></strong></div>
        <div>— Saturday, May 14, 2022</div>
    </header>
    <article>
        <h1 id=neural-nets-are-non-linear-linear-transformations>Neural nets are non-linear linear transformations</h1>
        <h2 id=traditional-teaching>Traditional teaching</h2>
        <p><strong><em>Skip to the next section if you’re unfamiliar with the underlying neural network
                    math</em></strong></p>
        <p>The neural network is a fundamental idea that powers many AI systems we have today. It has been scaled up to
            tackle hard problems such as natural language understanding, object recognition, driving, and many other
            abilities thought to be exclusive only to humans.</p>
        <p>(<em>The idea that intelligence is exclusive to a carbon-based neural network such as the brain is absurd.
                Any sort of computation that the brain performs to transform electrical signals into coherent thought
                can and will be done by silicon. The problem is figuring out the computations in the first place.</em>)
        </p>
        <p>Tangent aside, the following is a multi-layer feedforward neural network as it is commonly visualized, and
            how they’re commonly explained: <img
                src="https://blog.francisdb.net/assets/articles/nnets-nonlinear-transformations/nn.png"><br> A single
            “neuron” in this network has a “weight” and a “bias”. The neuron takes in a real-valued number as input, and
            transforms it according to the following equation: <span class="math inline">\(n(x) = n_{w} \cdot x +
                n_{b}\)</span> (<em>see how it’s a linear equation? This is important for later)</em>. It then feeds the
            result to the next layer.</p>
        <p>Thinking about neural networks this way is messy. Instead, we take a different route. Instead, let’s think
            about the input as being a vector, and we pass this vector input to each layer as a whole, instead of
            thinking about passing separate numbers to each neuron.</p>
        <p>Then, if you work out the math, you’ll realize that the linear transformations being performed by each
            neuron, if you consider them as a whole, is analogous to a matrix vector product. In fact, it is
            <em>exactly</em> a matrix vector product - we aggregate the weight vectors in each neuron to one large
            weight matrix, and each bias in each neuron to one large bias vector. Then, the computation being done by a
            single layer can be represented elegantly with this one-liner: <span class="math inline">\(l(x) = l_W \cdot
                x + l_b\)</span> - where <span class="math inline">\(l_W\)</span> is the weight matrix, and <span
                class="math inline">\(l_b\)</span> is the bias vector.
        </p>
        <p>Then, neural networks can be simply thought of as a series of linear transformations to data!</p>
        <h2 id=why-linear-transformations-are-useful>Why linear transformations are useful</h2>
        <p>A linear transformation is a function that takes in a coordinate, and maps it to a different coordinate in
            Euclidean space. However, the transformation is only classified as linear if grid lines remain
            <strong>parallel</strong> and <strong>evenly spaced</strong>.
        </p>
        <p>Matrices are linear transformations - performing a matrix vector product with a 2x2 matrix on a 2-vector
            results in another 2-vector. 2-vectors are just vectors with 2 dimensions; therefore, you can extend the
            idea of a coordinate to a 2-vector.</p>
        <p>We can combine linear transformations to perform prediction on data.</p>
        <p>For example, take the following scatter plot: <img
                src="https://blog.francisdb.net/assets/articles/nnets-nonlinear-transformations/plot1.png""><br><p>If we wanted to come up with our own series of linear transformations to categorise this data, we might come up with the following: <ol><li><strong>Rotation</strong> Imagine the linear boundary between both groups. We rotate the data such that that linear boundary is now horizontal: 
                    <img src="https://blog.francisdb.net/assets/articles/nnets-nonlinear-transformations/plot2.png"><br></li>
            <li><strong>Translation</strong> Now, with the rotation being done, our next transformation is not actually
                a linear transformation, but rather a translation along the y-axis such that both groups are separated
                by the y-axis, through the use of a bias vector instead of a weight matrix: <img
                    src="https://blog.francisdb.net/assets/articles/nnets-nonlinear-transformations/plot3.png"></li>
            <li> “<strong>Squeeze</strong>” Now, we use a singular matrix to transform the data from 2D Euclidean space
                to 1D Euclidean space, preserving the y-coordinate: <img
                    src="https://blog.francisdb.net/assets/articles/nnets-nonlinear-transformations/plot4.png"><br></br>
                Now, our hypothetical neural
                network’s outputs can be easily interpretable - anything less than 0 is part of the “red” class, and
                anything above 0 is part of the “blue” class.<p>These three linear transformations don’t need to be in
                    separate layers, either - they can be easily combined to one layer, by multiplying them all: <span
                        class="math inline">\(L = l_3 \cdot l_2 \cdot l_1\)</span>.
                <h2 id=but-how-about-more-complex-data>
            </li>
            </ol>
        </p>
        <h2>But how about more complex data?</h2>
        <p>Not all data is linearly separable by default. For example, observe this scatter plot: <img
                src="https://blog.francisdb.net/assets/articles/nnets-nonlinear-transformations/spiral.png"> There
            is no obvious linear pattern we can observe here. Therefore, we have to <strong>make</strong> our linear
            transformations <strong>non-linear</strong>, and we do that by interspersing them with nonlinear functions:
            <span class="math display">\[l(x) = \sigma(l_W \cdot x + l_b)\]</span> where <span
                class="math inline">\(\sigma\)</span> is any arbitrary nonlinear function.
        </p>
        <p>With this added functionality, here’s a visualization of how a nonlinear function can
            <strong>transform</strong> nonlinear data into linear data:
        </p>
        <p><img src="https://blog.francisdb.net/assets/articles/nnets-nonlinear-transformations/spiral.gif"></p>
        <p>Linearly separable data is much easier to deal with! </p>
        <h3 id=conclusion>Conclusion</h3>
        <p>In this article, we showed that neural networks (at least, the feed-forward neural network) are not what they
            are commonly compared to - biological neural networks. Instead, they are a series of linear transformations
            in the form of matrix vector products, interspersed with nonlinearities. </p>
        <p>This fundamental idea of composing “non-linear linear transformations” has led to many breakthroughs in the
            field of AI; namely, <a
                href=https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf>convolutional
                neural networks</a> in the field of computer vision, <a
                href=https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>transformer
                networks</a> in the field of natural language understanding, and <a
                href=http://www.bioinf.jku.at/publications/older/2604.pdf>LSTMs</a> in the field of general time-series
            forecasting. With parameter counts reaching up to the hundreds of <strong>billions</strong>, it is not hard
            to see that this basic idea of stacking transformations can tackle much higher-dimensional data such as
            images and natural language. </p>
    </article>
</main>